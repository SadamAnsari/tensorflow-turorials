{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import seaborn as sns\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('socialmedia-disaster-tweets.csv', encoding='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778243823</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>778243824</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>778243825</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>778243826</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778243827</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  778243823     True      golden                 156               NaN   \n",
       "1  778243824     True      golden                 152               NaN   \n",
       "2  778243825     True      golden                 137               NaN   \n",
       "3  778243826     True      golden                 136               NaN   \n",
       "4  778243827     True      golden                 138               NaN   \n",
       "\n",
       "  choose_one  choose_one:confidence choose_one_gold keyword location  \\\n",
       "0   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "1   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "2   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "3   Relevant                 0.9603        Relevant     NaN      NaN   \n",
       "4   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "\n",
       "                                                text  tweetid  userid  \n",
       "0                 Just happened a terrible car crash      1.0     NaN  \n",
       "1  Our Deeds are the Reason of this #earthquake M...     13.0     NaN  \n",
       "2  Heard about #earthquake is different cities, s...     14.0     NaN  \n",
       "3  there is a forest fire at spot pond, geese are...     15.0     NaN  \n",
       "4             Forest fire near La Ronge Sask. Canada     16.0     NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(labels=['_unit_id', '_golden', '_unit_state', '_trusted_judgments', '_last_judgment_at', 'choose_one:confidence', 'choose_one_gold', 'keyword', 'location', 'tweetid', 'userid'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert String category variables to Integer category variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.choose_one = tweets.choose_one.replace(to_replace=['Relevant', 'Not Relevant', 'Can\\'t Decide'], value=[1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>choose_one</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   choose_one                                               text\n",
       "0           1                 Just happened a terrible car crash\n",
       "1           1  Our Deeds are the Reason of this #earthquake M...\n",
       "2           1  Heard about #earthquake is different cities, s...\n",
       "3           1  there is a forest fire at spot pond, geese are...\n",
       "4           1             Forest fire near La Ronge Sask. Canada"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "\n",
    "all_tweets = tweets.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14325\n"
     ]
    }
   ],
   "source": [
    "for tweet in all_tweets:\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if word.isalnum() and word.lower() not in vocabulary:\n",
    "            vocabulary.append(word.lower())\n",
    "vocabulary = sorted(vocabulary)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14326\n"
     ]
    }
   ],
   "source": [
    "pad_char = \"$0$\"\n",
    "if pad_char not in vocabulary:\n",
    "    vocabulary.append(pad_char)\n",
    "vocab_size = len(vocabulary)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab_dict = {}\n",
    "for i, word in enumerate(vocabulary):\n",
    "    vocab_dict[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14326"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '02', '03', '04', '05', '05th', '06', '061', '06jst', '08'] $0$\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary[0:10], vocabulary[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just keep the columns with features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets.drop(labels=['choose_one'], axis=1).values\n",
    "y = tweets.choose_one.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Just happened a terrible car crash']\n",
      " ['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all']\n",
      " ['Heard about #earthquake is different cities, stay safe everyone.']\n",
      " ['there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all']\n",
      " ['Forest fire near La Ronge Sask. Canada']\n",
      " [\"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"]\n",
      " ['13,000 people receive #wildfires evacuation orders in California ']\n",
      " ['Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school ']\n",
      " ['#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires']\n",
      " ['Apocalypse lighting. #Spokane #wildfires']] [1 1 1 1 1 1 1 1 1 1]\n",
      "10876\n"
     ]
    }
   ],
   "source": [
    "print(X[:10], y[:10])\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x[0].split() for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Just', 'happened', 'a', 'terrible', 'car', 'crash'], ['Our', 'Deeds', 'are', 'the', 'Reason', 'of', 'this', '#earthquake', 'May', 'ALLAH', 'Forgive', 'us', 'all'], ['Heard', 'about', '#earthquake', 'is', 'different', 'cities,', 'stay', 'safe', 'everyone.'], ['there', 'is', 'a', 'forest', 'fire', 'at', 'spot', 'pond,', 'geese', 'are', 'fleeing', 'across', 'the', 'street,', 'I', 'cannot', 'save', 'them', 'all'], ['Forest', 'fire', 'near', 'La', 'Ronge', 'Sask.', 'Canada'], ['All', 'residents', 'asked', 'to', \"'shelter\", 'in', \"place'\", 'are', 'being', 'notified', 'by', 'officers.', 'No', 'other', 'evacuation', 'or', 'shelter', 'in', 'place', 'orders', 'are', 'expected'], ['13,000', 'people', 'receive', '#wildfires', 'evacuation', 'orders', 'in', 'California'], ['Just', 'got', 'sent', 'this', 'photo', 'from', 'Ruby', '#Alaska', 'as', 'smoke', 'from', '#wildfires', 'pours', 'into', 'a', 'school'], ['#RockyFire', 'Update', '=>', 'California', 'Hwy.', '20', 'closed', 'in', 'both', 'directions', 'due', 'to', 'Lake', 'County', 'fire', '-', '#CAfire', '#wildfires'], ['Apocalypse', 'lighting.', '#Spokane', '#wildfires']] [1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(X[:10], y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_trimmed_glove_vectors(vocab):\n",
    "    glove_filename  = 'glove.6B.300d.txt'\n",
    "    trimmed_filename = 'trimmed_gloves.npz'\n",
    "    dim = 300\n",
    "    if os.path.isfile(trimmed_filename):\n",
    "        return\n",
    "    \n",
    "    embeddings = np.zeros([len(vocab), dim])\n",
    "    with open(glove_filename, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            embedding = [float(x) for x in line[1:]]\n",
    "            if word in vocab:\n",
    "                word_idx = vocab[word]\n",
    "                embeddings[word_idx] = np.asarray(embedding)\n",
    "\n",
    "    np.savez_compressed(trimmed_filename, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trimmed_glove_vectors():\n",
    "    filename = 'trimmed_gloves.npz'\n",
    "    with np.load(filename) as data:\n",
    "        return data[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_trimmed_glove_vectors(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_trimmed_glove_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_vocab(lines):\n",
    "    X_onehot = []\n",
    "\n",
    "    for line in lines:\n",
    "        temp_X = np.array([vocabulary.index(word.lower()) for word in line if word.isalnum() or word == pad_char])\n",
    "        temp_oh_X = np.zeros((len(temp_X), vocab_size))\n",
    "        temp_oh_X[np.arange(len(temp_X)),temp_X] = 1\n",
    "        X_onehot.append(temp_oh_X)\n",
    "        \n",
    "    return X_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_word_ids(lines):\n",
    "    X_word_ids = []\n",
    "\n",
    "    for line in lines:\n",
    "        temp_X = np.array([vocabulary.index(word.lower()) for word in line if word.isalnum() or word == pad_char])\n",
    "        X_word_ids.append(temp_X)\n",
    "        \n",
    "    return X_word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 6966,  5834,   469, 12665,  2308,  3196]),\n",
       " array([ 9134,  3524,  1076, 12718, 10350,  8931, 12768,  7948,   814,\n",
       "         5137, 13429,   813])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_word_ids([X[0], X[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_max(X):\n",
    "    padded_X = []\n",
    "    cleared_X = []\n",
    "    \n",
    "    for x in X:\n",
    "        tempx = [word for word in x if word.isalnum()]\n",
    "        cleared_X.append(tempx)\n",
    "    \n",
    "    cleaned_lengths = [len(x) for x in cleared_X]\n",
    "    max_len = max(cleaned_lengths)\n",
    "    print(\"max_len\", max_len)\n",
    "    for x in cleared_X:\n",
    "        tempx = [word for word in x if word.isalnum()]\n",
    "        tempx.extend([pad_char]*(max_len-len(x)-1))\n",
    "        padded_X.append(tempx)\n",
    "    \n",
    "    return padded_X, cleaned_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_axis_1(data, ind):\n",
    "\n",
    "    batch_range = tf.range(tf.shape(data)[0])\n",
    "    indices = tf.stack([batch_range, ind], axis=1)\n",
    "    res = tf.gather_nd(data, indices)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(feed_X, feed_Y):\n",
    "    feed_dict = {}\n",
    "    feed_dict['data'], feed_dict['seq_length'] = pad_to_max(feed_X)\n",
    "    feed_dict['data'] = string_to_word_ids(feed_dict['data'])\n",
    "    feed_dict['labels'] = feed_Y\n",
    "    \n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 19\n",
      "[['Just', 'happened', 'a', 'terrible', 'car', 'crash', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$'], ['Our', 'Deeds', 'are', 'the', 'Reason', 'of', 'this', 'May', 'ALLAH', 'Forgive', 'us', 'all', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$'], ['Heard', 'about', 'is', 'different', 'stay', 'safe', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$'], ['there', 'is', 'a', 'forest', 'fire', 'at', 'spot', 'geese', 'are', 'fleeing', 'across', 'the', 'I', 'cannot', 'save', 'them', 'all', '$0$'], ['Forest', 'fire', 'near', 'La', 'Ronge', 'Canada', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$'], ['All', 'residents', 'asked', 'to', 'in', 'are', 'being', 'notified', 'by', 'No', 'other', 'evacuation', 'or', 'shelter', 'in', 'place', 'orders', 'are', 'expected'], ['people', 'receive', 'evacuation', 'orders', 'in', 'California', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$'], ['Just', 'got', 'sent', 'this', 'photo', 'from', 'Ruby', 'as', 'smoke', 'from', 'pours', 'into', 'a', 'school', '$0$', '$0$', '$0$', '$0$'], ['Update', 'California', '20', 'closed', 'in', 'both', 'directions', 'due', 'to', 'Lake', 'County', 'fire', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$'], ['Apocalypse', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$', '$0$']]\n",
      "[6, 12, 6, 17, 6, 19, 6, 14, 12, 1]\n"
     ]
    }
   ],
   "source": [
    "print_x = pad_to_max(X[0:10])\n",
    "\n",
    "for x in print_x:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 19\n",
      "{'data': [array([ 6966,  5834,   469, 12665,  2308,  3196, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 9134,  3524,  1076, 12718, 10350,  8931, 12768,  7948,   814,\n",
      "        5137, 13429,   813, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 5941,   518,  6729,  3773, 12081, 10978, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([12737,  6729,   469,  5128,  4959,  1196, 11956,  5402,  1076,\n",
      "        5025,   578, 12718,  6317,  2277, 11072, 12725,   813, 14325]), array([ 5128,  4959,  8600,  7222, 10842,  2259, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([  813, 10610,  1157, 12890,  6453,  1076,  1579,  8801,  2176,\n",
      "        8730,  9123,  4536,  9075, 11393,  6453,  9617,  9087,  1076,\n",
      "        4625]), array([ 9435, 10367,  4536,  9087,  6453,  2220, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 6966,  5569, 11259, 12768,  9520,  5247, 10913,  1132, 11700,\n",
      "        5247,  9800,  6658,   469, 11119, 14325, 14325, 14325, 14325]), array([13390,  2220,   157,  2722,  6453,  1891,  3802,  4119, 12890,\n",
      "        7241,  3147,  4959, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 1007, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325])], 'seq_length': [6, 12, 6, 17, 6, 19, 6, 14, 12, 1], 'labels': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "print(get_feed_dict(X[0:10], y[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10876 9788\n"
     ]
    }
   ],
   "source": [
    "print(len(X), round(len(X) * 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select indexes to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.random.choice(len(X), round(len(X) * 0.9), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7405 10573  2013 ...    78  4420  9733] 9788\n"
     ]
    }
   ],
   "source": [
    "print(train_index, len(train_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = np.array(list(set(range(len(X))) - set(train_index)))\n",
    "X = np.array(X)\n",
    "train_X = X[train_index]\n",
    "train_y = y[train_index]\n",
    "test_X = X[test_index]\n",
    "test_y = y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4098 8194 2052 ... 2043 4092 2046] 1088\n"
     ]
    }
   ],
   "source": [
    "print(test_index, len(test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([list([\"I'm\", 'about', 'to', 'be', 'obliterated']),\n",
       "        list(['#Insurance:', 'Texas', 'Seeks', 'Comment', 'on', 'Rules', 'for', 'Changes', 'to', 'Windstorm', 'Insurer', 'http://t.co/rb02svlpPu']),\n",
       "        list(['lets', 'hope', 'this', 'concert', 'ends', 'with', 'zero', 'casualties', 'amen']),\n",
       "        list(['Storm', 'in', 'RI', 'worse', 'than', 'last', 'hurricane.', 'My', 'city&amp;3others', 'hardest', 'hit.', 'My', 'yard', 'looks', 'like', 'it', 'was', 'bombed.', 'Around', '20000K', 'still', 'without', 'power']),\n",
       "        list(['Tell', '@BarackObama', 'to', 'rescind', 'medals', 'of', \"'honor'\", 'given', 'to', 'US', 'soldiers', 'at', 'the', 'Massacre', 'of', 'Wounded', 'Knee.', 'SIGN', 'NOW', '&amp;', 'RT!', 'https://t.co/u4r8dRiuAc']),\n",
       "        list([\"I'm\", 'more', 'into', 'the', 'healing/reviving', 'side', 'of', 'the', 'game', 'rather', 'than', 'better', 'attacking', 'so', 'for', 'now', 'Siren', '&gt;', 'all', 'other', 'characters', '(except', 'new', 'girl).']),\n",
       "        list(['Remove', 'the', 'http://t.co/OP2NObtoul', 'and', 'Linkury', 'Browser', 'Hijacker', 'http://t.co/JjKlf2PlX7', 'http://t.co/t3OQgw0Ce7']),\n",
       "        list(['Wreckage', 'is', 'MH370:', 'Najib', 'http://t.co/iidKC0jSBx', '#MH370', '#najibrazak', '#MalaysiaAirlines']),\n",
       "        list(['Detonation', 'fashionable', 'mountaineering', 'electronic', 'watch', 'water-resistant', 'couples', 'leisure', 'tab‰Û_', 'http://t.co/g6hjTj3SDy', 'http://t.co/yydEghGP64']),\n",
       "        list(['FB', 'page', 'of', 'Bushman', \"Safari's\", 'Zimbabwe', 'the', 'company', 'that', 'Palmer', 'used', '2', 'kill', 'Cecil', 'is', 'inundated', 'with', 'negative', 'commentes', 'https://t.co/QwIIhNMChR'])],\n",
       "       dtype=object), array([0, 0, 0, 1, 1, 0, 0, 1, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0:10], train_y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Tensorflow Graph\n",
    "Let's start building Tensorflow model. Tensorflow is a graph based framework. So we have to build the whole graph of our model before we start any kind of training. For most ML problems, the graph building process is pretty straight forward.\n",
    "\n",
    "- Step 1: Define placeholders for input Data and Labels.\n",
    "- Step 2: Pass the Data through the layers.\n",
    "- Step 3: Define a loss function.\n",
    "- Step 4: Apply Gradient Descent (or other optimisers) on the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders are special type of Tensorflow variables. These variables don't hold any particular values and are used as input nodes to the graph. The data is fed to these variables in order to Train the model or infer using the model.\n",
    "\n",
    "- \"data\" is used for data input. The shape is Batch Size x Input Features. \n",
    "- \"labels\" is used for output labels. Batch Size x Output Space\n",
    "\n",
    "To keep the batch size \"variable\", we use None instead of a specific Batch Size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.placeholder(dtype=tf.int32, shape=[None, None], name='Data_Input')\n",
    "labels = tf.placeholder(dtype=tf.float32, shape=[None, 1], name='Labels_Input')\n",
    "seq_lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='Seq_Lengths')\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, shape=[], name=\"dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_matrix = tf.Variable(embeddings, name=\"word_embeddings\", dtype=tf.float32, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Data_Input:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass the data through all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = tf.nn.embedding_lookup(word_embeddings_matrix, data, name=\"input_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_lstm = 32\n",
    "\n",
    "cell_fw = tf.contrib.rnn.LSTMCell(size_lstm)\n",
    "cell_bw = tf.contrib.rnn.LSTMCell(size_lstm)\n",
    "\n",
    "(output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw, cell_bw, input_embeddings,\n",
    "    sequence_length=seq_lengths, dtype=tf.float32)\n",
    "\n",
    "lstm = tf.concat([output_fw, output_bw], axis=-1)\n",
    "output = tf.nn.dropout(lstm, keep_prob)\n",
    "\n",
    "last_output = extract_axis_1(output, seq_lengths - 1)\n",
    "last_output = tf.reshape(last_output, [-1, 2*size_lstm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(last_output, units=1,\n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                         kernel_regularizer=tf.nn.l2_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the logits calculated, lets add the loss operation. Since we're doing logistic regression, the loss function is sigmoid cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "tf.summary.scalar('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "batch_size = 32\n",
    "iter_num = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we iterate on the parameter values such that the loss is minimized. To reduce the loss, we have multiple Optimizers available. Let's use Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction and Accuracy utility operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the accuracy\n",
    "# The default threshold is 0.5, rounded off directly\n",
    "prediction = tf.round(tf.sigmoid(logits), name='Prediction')\n",
    "# Bool into float32 type\n",
    "correct = tf.cast(tf.equal(prediction, labels), dtype=tf.float32)\n",
    "# Average\n",
    "accuracy = tf.reduce_mean(correct, name='Accuracy')\n",
    "\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "# End of the definition of the model framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_trace = []\n",
    "train_acc = []\n",
    "test_acc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / Evaluating the model\n",
    "Up until now, we have just built the graph of model in Tensorflow. Now we will start trainig and evaluating the model. To use the model:\n",
    "- Initialise the model.\n",
    "- To \"Train\" the model: feed data and labels and process the model till the \"goal\" operation.\n",
    "- To \"Evaluate\" the model: provide data and lables and process till \"accuracy\" operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation is pretty standard. We get a session object after initialisation and this session object is used to interact with the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('./text_classification_glove_logs', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model:\n",
    "- Get a batch of data and labels.\n",
    "- Feed data and labels to the graph.\n",
    "- Check the Train and Test data accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 28\n",
      "[array([13949,  2129,  9009,  2220,  2579,  4212,  8646, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 8338,   913,  1673,  6894,  1076,   469,  9936,  6453, 12768,\n",
      "        5343,  1829,  1378,  8701, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([13189,  1349, 12890,  6317,  5900, 10593, 14247,   913, 12451,\n",
      "        1316, 14254, 11551,  1132, 12777, 12745, 13860,  6732,  3027,\n",
      "        4407, 13537, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 1585,  6753,  9075,  8789,  5761, 12930,  8435, 10232,  9134,\n",
      "        8663,  9636,  7845, 13076,  1076,   602, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 5267,  1102,   913,  4084, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([13931, 12745, 12522,  1349, 12890,   677,  9010, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 6753, 14101, 13189,  6658,   469, 11033,  6362, 10750,  5381,\n",
      "         469,  5549,  6599, 12745,  5381,  4127,  1196,  7353,  8656,\n",
      "        2704,  6729, 12737, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 9714,  8945, 14103, 12410,  3469,   682,  4589, 10712,  9714,\n",
      "        8945, 14103, 12410,  7091,   682,  4587,  8931,  5724, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 6317,  2124,   239,  8931,  8505, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 5985,  6729, 12395,  2176,  3677, 11746,  1511,  2324,   913,\n",
      "        7412, 14254,  3677,  3048, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 9370,  3188, 10798, 13629, 10128,  6751,  7781, 13739,  2155,\n",
      "        6729,  6753, 12930,  7296, 12890,  4487,   469, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([10212,  1919, 10155,  1143,  9752, 13754,  4534, 13892,  4330,\n",
      "       12602,  3831, 10208, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 1707,   913, 10907, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 2220, 13950,  5115, 12781, 12890, 13548, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 4748, 12890, 12290,  9165,  3838,  9140,  6453,  4223,  1501,\n",
      "        8666, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([12718, 13710,  6747,  4471,  5877, 12147, 12305, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 6729,  6753,  9773, 12890, 11726,  6658,   469,  6200, 11746,\n",
      "        6317,  2258, 12009,  8512, 14012,   469,  5911,  8615,   913,\n",
      "        6552, 11788,  3206,  3838,  6658,  8505, 13517, 13379,  6317,\n",
      "        3762]), array([ 4330, 11482, 12486,  4232, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 4748, 12890, 12290,  9165,  8343, 12705,   292,  4748,   662,\n",
      "        2176, 12718,  4789,  9140,  8931, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([13265,  9858, 14257,  5110,   986,  9165, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 6729,  3803, 10632,  5110, 12718,  2807,  6453,  4394, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 3862,  9480,  5514,  8727, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 6077, 12406,  4906,  8832,  6515,  6453,  1955, 13567, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([14247,  1538,  3610,  1076,  5075,  9009,  5047,  9134,  1881,\n",
      "       14012,  6387,  6417, 12890,   612, 12890, 12724, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 3482,  6317,  2037,  6753, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 5423,  3436, 12027, 12096,  1932,   913, 13912,  3745,  5462,\n",
      "        1919, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([11267,    79,  3538,  1196, 12718,  4448,  5982, 14204, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 1490,   291,  5298,  8297,  4122,  8931,  5910, 13297, 11775,\n",
      "        7942, 13548, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 3760,  5869, 12402, 12718, 11465, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([14012,   813,  4119, 10619, 14186, 14099, 13242,  2916,  3868,\n",
      "         813,  4690,   551,  9009, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([12890,   813, 12718,  4864,  8931, 12718, 14082, 10750,  5658,\n",
      "        5877,  1109, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325]), array([ 6317,  6177,  6317,  5437,  4285, 12894,  1196, 14071, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325,\n",
      "       14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325, 14325])]\n",
      "[7, 13, 20, 15, 4, 7, 21, 17, 5, 13, 16, 12, 3, 6, 10, 7, 28, 4, 14, 6, 8, 4, 8, 16, 4, 10, 8, 11, 5, 13, 11, 8]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-da9b165318cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                                      \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                                      \u001b[0mseq_lengths\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'seq_length'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                                                      keep_prob: 0.5})\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     temp_loss = sess.run(loss, feed_dict={data: feed_dict['data'],\n",
      "\u001b[1;32mc:\\users\\sansari\\documents\\tensorflow\\nlp_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sansari\\documents\\tensorflow\\nlp_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1077\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mc:\\users\\sansari\\documents\\tensorflow\\nlp_env\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# training model\n",
    "for epoch in range(iter_num):\n",
    "    # Generate random batch index\n",
    "    batch_index = np.random.choice(len(train_X), size=batch_size)\n",
    "    batch_train_X = train_X[batch_index]\n",
    "    batch_train_y = np.matrix(train_y[batch_index]).T\n",
    "    feed_dict = get_feed_dict(batch_train_X, batch_train_y)\n",
    "    \n",
    "    print(feed_dict['data'])\n",
    "    print(feed_dict['seq_length'])\n",
    "    _, summary = sess.run([goal, merged], feed_dict={data: feed_dict['data'], \n",
    "                                                     labels: feed_dict['labels'],\n",
    "                                                     seq_lengths: feed_dict['seq_length'],\n",
    "                                                     keep_prob: 0.5})\n",
    "\n",
    "    temp_loss = sess.run(loss, feed_dict={data: feed_dict['data'],\n",
    "                                          labels: feed_dict['labels'],\n",
    "                                          seq_lengths: feed_dict['seq_length'],\n",
    "                                          keep_prob: 0.5})\n",
    "\n",
    "    loss_trace.append(temp_loss)\n",
    "    \n",
    "    train_writer.add_summary(summary, epoch)\n",
    "    \n",
    "    # print('epoch: {:4d} loss: {:5f}'.format(epoch + 1, temp_loss))\n",
    "    # output\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        feed_dict = get_feed_dict(test_X, np.matrix(test_y).T)\n",
    "\n",
    "        test_batch_acc = sess.run(accuracy, feed_dict={data: feed_dict['data'],\n",
    "                                              labels: feed_dict['labels'],\n",
    "                                              seq_lengths: feed_dict['seq_length'],\n",
    "                                              keep_prob: 1})\n",
    "        test_acc.append(test_batch_acc)\n",
    " \n",
    "        print('epoch: {:4d} loss: {:5f} test_acc: {:5f}'.format(epoch + 1, temp_loss, test_batch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the results\n",
    "# loss function\n",
    "plt.plot(loss_trace)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc, 'b-', label='train accuracy')\n",
    "plt.plot(test_acc, 'k-', label='test accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Train and Test Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}